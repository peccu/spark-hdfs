# based on https://github.com/bitnami/containers/blob/main/bitnami/spark/docker-compose.yml
# and merge https://github.com/mrn-aglic/pyspark-playground/blob/main/docker-compose.yml

version: '3.8'

x-volumes: &volumes
  # samples are in /opt/bitnami/spark/examples
  - ./docker/jupyterlab:/requirements
  - ./jupyter-volume:/opt/spark
  - ./spark-volume/apps:/opt/spark/apps
  - ./spark-volume/conf/spark-defaults.conf:/opt/bitnami/spark/conf/spark-defaults.conf
  - ./spark-volume/data:/opt/spark/data
  - spark-logs:/opt/spark/spark-events

x-worker: &worker
  profiles: ['all', 'spark']
  user: root
  image: docker.io/bitnami/spark:3.5
  depends_on:
    - spark
  restart: always
  volumes: *volumes

services:

  ################
  # Interface
  ################

  # Jupyter
  jupyter:
    profiles: ['all', 'spark']
    user: root
    # build:
    #   context: docker/jupyterlab
    # image: peccu/spark-jupyterlab
    image: jupyterlab
    environment:
      - SPARK_MASTER=spark:://spark:7077
      - SPARK_USER=root
      - SPARK_DAEMON_USER=root
    restart: always
    depends_on:
      - spark
    ports:
      - '28888:8888'
    volumes: *volumes

  ################
  # Spark
  ################

  spark:
    profiles: ['all', 'spark']
    # https://hub.docker.com/r/bitnami/spark
    image: docker.io/bitnami/spark:3.5
    user: root
    hostname: spark
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_MASTER_WEBUI_PORT=2080
      - SPARK_USER=root
      - SPARK_DAEMON_USER=root
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:2080" ]
      interval: 5s
      timeout: 3s
      retries: 3
    restart: always
    ports:
      - '2080:2080'
      - '4040:4040'
    volumes: *volumes

  worker1:
    << : *worker
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark:7077
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=1
      - SPARK_USER=root
      - SPARK_DAEMON_USER=root
      # for fixing web ui url
      - SPARK_PUBLIC_DNS=localhost
      - SPARK_WORKER_WEBUI_PORT=28081
      - SPARK_MASTER_HOST=spark
      - SPARK_MASTER_WEBUI_PORT=2080
    ports:
      - '28081:28081'

  # worker2:
  #   << : *worker
  #   environment:
  #     - SPARK_MODE=worker
  #     - SPARK_MASTER_URL=spark://spark:7077
  #     - SPARK_RPC_AUTHENTICATION_ENABLED=no
  #     - SPARK_RPC_ENCRYPTION_ENABLED=no
  #     - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
  #     - SPARK_WORKER_MEMORY=1G
  #     - SPARK_WORKER_CORES=1
  #     - SPARK_USER=root
  #     - SPARK_DAEMON_USER=root
  #     # for fixing web ui url
  #     - SPARK_PUBLIC_DNS=localhost
  #     - SPARK_WORKER_WEBUI_PORT=28082
  #     - SPARK_MASTER_HOST=spark
  #     - SPARK_MASTER_WEBUI_PORT=2080
  #   ports:
  #     - '28082:28082'

  # worker3:
  #   << : *worker
  #   environment:
  #     - SPARK_MODE=worker
  #     - SPARK_MASTER_URL=spark://spark:7077
  #     - SPARK_RPC_AUTHENTICATION_ENABLED=no
  #     - SPARK_RPC_ENCRYPTION_ENABLED=no
  #     - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
  #     - SPARK_WORKER_MEMORY=1G
  #     - SPARK_WORKER_CORES=1
  #     - SPARK_USER=root
  #     - SPARK_DAEMON_USER=root
  #     # for fixing web ui url
  #     - SPARK_PUBLIC_DNS=localhost
  #     - SPARK_WORKER_WEBUI_PORT=28083
  #     - SPARK_MASTER_HOST=spark
  #     - SPARK_MASTER_WEBUI_PORT=2080
  #   ports:
  #     - '28083:28083'

  # worker4:
  #   << : *worker
  #   environment:
  #     - SPARK_MODE=worker
  #     - SPARK_MASTER_URL=spark://spark:7077
  #     - SPARK_RPC_AUTHENTICATION_ENABLED=no
  #     - SPARK_RPC_ENCRYPTION_ENABLED=no
  #     - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
  #     - SPARK_WORKER_MEMORY=1G
  #     - SPARK_WORKER_CORES=1
  #     - SPARK_USER=root
  #     - SPARK_DAEMON_USER=root
  #     # for fixing web ui url
  #     - SPARK_PUBLIC_DNS=localhost
  #     - SPARK_WORKER_WEBUI_PORT=28084
  #     - SPARK_MASTER_HOST=spark
  #     - SPARK_MASTER_WEBUI_PORT=2080
  #   ports:
  #     - '28084:28084'

  spark-history:
    image: docker.io/bitnami/spark:3.5
    profiles: ['all', 'history']
    entrypoint: ['start-history-server.sh']
    depends_on:
      - spark
    volumes:
      - spark-logs:/opt/spark/spark-events
    ports:
      - '18080:18080'

  ################
  # HDFS
  ################
  # # single container version
  # namenode:
  #   profiles: ['all', 'hdfs']
  #   build: docker/hdfs
  #   # image: peccu/hdfs
  #   image: hdfs
  #   command: single
  #   ports:
  #     # web ui
  #     - "9870:9870"
  #   volumes:
  #     - ./hdfs-shared-data:/hdfs-shared-data
  #     - ./hdfs-volume/namenode:/hadoop/dfs/namenode
  #   environment:
  #     - CLUSTER_NAME=cluster

  namenode:
    profiles: ['all', 'hdfs']
    # build: docker/hdfs
    # image: peccu/hdfs
    image: hdfs
    command: namenode
    ports:
      # web ui
      - "9870:9870"
    volumes:
      - ./hdfs-shared-data:/hdfs-shared-data
      - ./hdfs-volume/namenode:/hadoop/dfs/namenode
    environment:
      - CLUSTER_NAME=cluster

  datanode:
    profiles: ['all', 'hdfs']
    # build: docker/hdfs
    # image: peccu/hdfs
    image: hdfs
    command: datanode
    ports:
      # web ui
      - "9864:9864"
    volumes:
      - ./hdfs-volume/datanode:/hadoop/dfs/datanode
    depends_on:
      - namenode
    environment:
      - CLUSTER_NAME=cluster
      - SERVICE_PRECONDITION=namenode:9870

  ################
  # Hive
  ################
  hive-metastore:
    profiles: ['all', 'hive']
    # build:
    #   context: docker/hive
    #   dockerfile: Dockerfile.metastore
    image: peccu/hive-metastore
    image: metastore
    ports:
      - "9083:9083"

  hive-server:
    profiles: ['all', 'hive']
    # build:
    #   context: docker/hive
    #   dockerfile: Dockerfile.server
    # image: peccu/hive-server
    image: hive-server
    ports:
      - "10000:10000"
    depends_on:
      - hive-metastore

volumes:
  spark-logs:
